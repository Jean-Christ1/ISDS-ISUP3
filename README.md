# ISDS (ISUP et Sorbonne Universit√©)
Ing√©nierie, Statistique et Data Science (ISDS) du master Ing√©nieris Math√©matiques Appliqu√©es de Sorbonne Universit√© et d'ISUP.<br/>

üîó[https://isup.sorbonne-universite.fr/formations/filiere-ingenierie-statistique-et-data-science-isds] <br/>
üîó[https://sciences.sorbonne-universite.fr/formation-sciences/masters/master-mathematiques-et-applications/m2-parcours-ingenierie-mathematique]

<a id="top"></a>
<div class="list-group" id="list-tab" role="tablist">
<h3 class="list-group-item list-group-item-action active" data-toggle="list" role="tab" aria-controls="home">PROGRAMME ISDS 2 - ISUP3</h3>
  
* [1. BLOC base              : UE - Ing√©nierie 1 et UE - Math√©matiques et mod√©lisation](#2)
* [2. BLOC fondamental       : UE - Ing√©nierie 2 et UE - Informatique pour l'ing√©nierie](#2)
* [3. BLOC de sp√©cialisation : UE - Sp√©cialisation 1 et UE - Sp√©cialisation 2](#2)

  ---------------
  
  <a id="2"></a>
<font color="darkslateblue" size=+2.5><b>[1. BLOC 1 : UE - Ing√©nierie 1 et UE - Math√©matiques et mod√©lisation]()</b></font>


[**Machine Learning**]() (prof. Claire Boyer) : 
  
  Ce cours pr√©sente les grands principes de l‚Äôapprentissage statistique et automatique et les principales m√©thodes de pr√©diction (classification et r√©gression), de clustering et de r√©duction de dimension. On s‚Äôattachera √† aborder l‚Äôapprentissage automatique d‚Äôun point de vue th√©orique mais aussi d‚Äôun point de vue algorithmique, puisque la plupart des concepts pourront s‚Äôillustrer par des travaux pratiques en Python.  Il a pour but de fournir les outils n√©cessaires √† :
- Identidier les probl√®mes qui peuvent √™tre r√©solus par des approches de Machine Learning ;
- Formaliser ces probl√®mes en termes de Machine Learning ;
- IdentiÔ¨Åer les algorithmes les plus appropri√©s pour ces probl√®mes et les mettre en ≈ìuvre afin d‚Äôen comprendre les tenants et aboutissants ;
- Evaluer et comparer de la mani√®re la plus objective possible les performances de plusieurs algorithmes de Machine Learning et du Deep Learning pour une application particuli√®re.
  
- Principe de minimisation du risque empirique, th√©orie de Vapnik-Chervonenkis;
- Optimisation pour le machine learning;
- Apprentissage supervis√©: m√©thodes param√©triques, √† noyaux et non param√©triques;
- Apprentissage non-supervis√©: clustering et r√©duction de dimension;
- Compl√©tion de matrice;
- Introduction au deep learning (pour l'UE de Sp√©cialisation).

  
[**Mod√®les al√©atoires**] (Prof. Olivier Bardou):  <br/> 
  
  Ce module a pour objectif d'aborder la mod√©lisation Markovienne. Ces processus sont tr√®s int√©ressants dans la mesure o√π ils poss√®dent de nombreuses applications. La d√©couverte de ces processus de Markov comme nous le voyons sous-entend une compr√©hension math√©matique du ph√©nom√®ne mais aussi une approche pragmatique gr√¢ce √† des exercices appliqu√©s √† des situations quotidiennes.
- Cha√Ænes de Markov √† temps discret;
- Processus de sauts markoviens;
- Propri√©t√©s des processus en temps long, th√©or√®mes ergodiques.
- etc.

[**Calcul stochastique**](https://www.lpsm.paris/pageperso/zhan) (Prof. Zhan Shi) :  
  
  L‚Äôobjet de la th√©orie des processus stochastiques est l‚Äô√©tude des ph√©nom√®nes al√©atoires d√©pendant du temps. Le but de ce cours est d'introduire les notions de martingales, de mouvement brownien et d'int√©grales stochastiques par rapport au mouvement brownien ainsi que les bases du calcul d'It√¥.
- Martingales √† temps discret, martingales √† temps continu, convergences et th√©or√®me d‚Äôarr√™t;
- Mouvement brownien, propri√©t√© de Markov et propri√©t√© de martingale; 
- Int√©grale stochastique par rapport au mouvement brownien, formule d‚ÄôIt√¥, th√©or√®me de Girsanov. 
- Introduction aux √©quations diff√©rentielles stochastiques, √©quations √† coefficients lipschitziens, diffusions et propri√©t√© de Markov.
 


  
[**TP C/C++**]() (Prof. Vincent Lemaire) : 
  
  Ma√Ætriser les principes fondamentaux de la conception objet et les pratiquer de fa√ßon effective en C++ au travers d‚Äôune application r√©alis√©e de fa√ßon it√©rative. Mettre en ≈ìuvre les nouveaut√©s offertes par la derni√®re norme C++ 11 / 14. Les diff√©rents aspects abord√©s pendant ce cours sont les suivants :
- Syntaxe classique du C/C++;
- Programmation orient√©e objets (classes, h√©ritage, polymorphisme dynamique) ;
- Programmation g√©n√©rique (Template, STL, polymorphisme statique);
- la programmation moderne du C++14 et l‚Äôint√©gration avec R via Rcpp et Python via pybind11; 
- Exemples num√©riques li√©s aux √©quations paraboliques (m√©thodes d√©terministes et al√©atoires).
 

[**M√©thodes Num√©riques**]() (Prof. Cindy Guichard) : 
  
  Ce cours traite de la discr√©tisation des  √©quation aux d√©riv√©es partielles (EDP) en 1D (une dimension) et 2D notamment par la m√©thode des diff√©rences finies.

  
<a href="#top" class="btn btn-primary btn-sm" role="button" aria-pressed="true" style="color:white" data-toggle="popover">Retour au programme</a>
 
  
  <a id="2"></a>
<font color="darkslateblue" size=+2.5><b>[2. BLOC fondamental : UE - Ing√©nierie 2 et UE - Informatique pour l'ing√©nierie]()</b></font>
   
[Mod√®les √† structures latente]()  (Prof. Jean-Patrick Baudry):  
  
  Ce module aborde l'ensemble des techniques d‚Äôexploration des donn√©es servant √† r√©sumer les informations sur les donn√©es ou √† d√©terminer des liens entre les points. Il a pour but principal de structurer les donn√©es en classes homog√®nes. C'est-√†-dire, regrouper les points (individus) en clusters ou classes de telles sortes que les donn√©es d‚Äôun cluster soient les plus similaires possibles. Ce cours, tout comme les cours cit√©s dans mes rapports pr√©c√©dant, y compris ce rapport et ceux √† venir, se veut pratique en proposant des exercices de TP par bin√¥me  et des application concr√®tes mis en ouvre principalement avec le logiciels R. Les notions abord√©es dans ce cours apportent des r√©ponses concr√®tes aux probl√©matiques li√©es :
- A la nature des observations (donn√©es) ;
- Au notion de similarit√© ou de dissimilarit√© entre observations ;
- Aux caract√©ristiques d‚Äôun cluster ;
- Au choix du nombre (optimal) de clusters et aux comparaisons de diÔ¨Ä√©rents r√©sultats de clustering ;
- au fonctionnement des algorithmes de clustering et au choix de ces algorithmes (ACP, Kmeans, CHA, Model-Based Clustering,  M√©thodes bay√©siennes, Markov chain Monte Carlo (MCMC methode)) ; 
- etc.
  
  
  
  
